{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from advertorch.attacks import LinfPGDAttack, L2PGDAttack, DDNL2Attack\n",
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (17, 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, activation=F.relu):\n",
    "        super(SmallCNN, self).__init__()\n",
    "        self.in_normalize = nn.BatchNorm2d(1)\n",
    "        self.conv1 = nn.Conv2d(1, 512, 17, padding=8, padding_mode='circular')\n",
    "        self.activation = activation\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.in_normalize(x)\n",
    "        x = self.activation(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 512*1)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class FCNN(nn.Module):\n",
    "    def __init__(self, activation=F.relu):\n",
    "        super(FCNN, self).__init__()\n",
    "        self.in_normalize = nn.BatchNorm2d(1)\n",
    "        self.fc1 = nn.Linear(input_shape[0]*input_shape[1], 512)\n",
    "        self.activation = activation\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.in_normalize(x)\n",
    "        x = x.view(-1, input_shape[0]*input_shape[1])\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_1 = np.zeros(input_shape)+0.5\n",
    "im_2 = im_1.copy()\n",
    "im_1[7][7]+=0.5\n",
    "im_2[7][7]-=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 17, 17)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack([im_1, im_2]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Train Acc: 100.000, Loss : 0.579\n",
      "Epoch: 200, Train Acc: 100.000, Loss : 0.419\n",
      "Epoch: 300, Train Acc: 100.000, Loss : 0.235\n",
      "Epoch: 400, Train Acc: 100.000, Loss : 0.117\n",
      "Epoch: 500, Train Acc: 100.000, Loss : 0.063\n",
      "Epoch: 600, Train Acc: 100.000, Loss : 0.039\n",
      "Epoch: 700, Train Acc: 100.000, Loss : 0.026\n",
      "Epoch: 800, Train Acc: 100.000, Loss : 0.019\n",
      "Epoch: 900, Train Acc: 100.000, Loss : 0.019\n",
      "Epoch: 1000, Train Acc: 100.000, Loss : 0.018\n",
      "Epoch: 1100, Train Acc: 100.000, Loss : 0.018\n",
      "Epoch: 1200, Train Acc: 100.000, Loss : 0.017\n",
      "Epoch: 1300, Train Acc: 100.000, Loss : 0.017\n",
      "Epoch: 1400, Train Acc: 100.000, Loss : 0.016\n",
      "Epoch: 1500, Train Acc: 100.000, Loss : 0.016\n",
      "Epoch: 1600, Train Acc: 100.000, Loss : 0.016\n",
      "Epoch: 1700, Train Acc: 100.000, Loss : 0.015\n",
      "Epoch: 1800, Train Acc: 100.000, Loss : 0.015\n",
      "Epoch: 1900, Train Acc: 100.000, Loss : 0.015\n",
      "Epoch: 2000, Train Acc: 100.000, Loss : 0.014\n"
     ]
    }
   ],
   "source": [
    "# im_1 = im_1.\n",
    "data = np.stack([np.expand_dims(im_1, axis=0), np.expand_dims(im_2, axis=0)], axis=0)\n",
    "y = np.array([1, 0])\n",
    "batch_X, batch_y = torch.tensor(data).float().cuda(), torch.tensor(y).long().cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model_cnn = model = SmallCNN()\n",
    "# model_fc = model = FCNN()\n",
    "model = model.cuda()\n",
    "LR = 0.01\n",
    "opt = torch.optim.SGD(model.parameters(), lr = LR)\n",
    "\n",
    "\n",
    "for idx in range(2000):\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    output = model(batch_X)\n",
    "    loss = criterion(output, batch_y)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    total_loss+=loss*batch_y.size(0)\n",
    "    total_acc += (output.max(1)[1] == batch_y).sum().item()\n",
    "    if (idx+1) % 800==0:\n",
    "        opt.param_groups[0].update(lr=LR/10)\n",
    "    if (idx+1)%100==0:\n",
    "        print(\"Epoch: {}, Train Acc: {:.3f}, Loss : {:.3f}\".format(idx+1, total_acc*100./y.shape[0], total_loss/y.shape[0]))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmallCNN(\n",
       "  (in_normalize): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv1): Conv2d(1, 512, kernel_size=(17, 17), stride=(1, 1), padding_mode=circular)\n",
       "  (pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FCNN(\n",
       "  (in_normalize): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=289, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = model_cnn.eval()\n",
    "model_fc = model_fc.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pgd(X, y, adversary, batch_size):\n",
    "    SPLITS = int(X.shape[0]/batch_size)\n",
    "    total_acc = 0\n",
    "    for idx in range(SPLITS):\n",
    "        s1, s2 = idx*batch_size, min((idx+1)*batch_size, X.shape[0])\n",
    "#         ipdb.set_trace()\n",
    "        batch_X, batch_y = X[s1:s2,:,:,:], y[s1:s2]\n",
    "        batch_X, batch_y = torch.tensor(batch_X).float().cuda(), torch.tensor(batch_y).long().cuda()\n",
    "#         ipdb.set_trace()\n",
    "        adv_untargeted = adversary.perturb(batch_X, batch_y)\n",
    "        robust_out = model(adv_untargeted)\n",
    "        total_acc += (robust_out.max(1)[1] == batch_y).sum().item()\n",
    "        \n",
    "#     print(\"Test Robust Acc: {:.3f}\".format(total_acc*100./y.shape[0]))\n",
    "    return total_acc*100./y.shape[0]\n",
    "\n",
    "def test_pgd_ddn(X, y, adversary, batch_size):\n",
    "    SPLITS = int(X.shape[0]/batch_size)\n",
    "    total_acc = 0\n",
    "    for idx in range(SPLITS):\n",
    "        s1, s2 = idx*batch_size, min((idx+1)*batch_size, X.shape[0])\n",
    "#         ipdb.set_trace()\n",
    "        batch_X, batch_y = X[s1:s2,:,:,:], y[s1:s2]\n",
    "        batch_X, batch_y = torch.tensor(batch_X).float().cuda(), torch.tensor(batch_y).long().cuda()\n",
    "#         ipdb.set_trace()\n",
    "        adv_untargeted = adversary.perturb(batch_X, batch_y)\n",
    "        robust_out = model(adv_untargeted)\n",
    "        total_acc += (robust_out.max(1)[1] == batch_y).sum().item()\n",
    "        \n",
    "#     print(\"Test Robust Acc: {:.3f}\".format(total_acc*100./y.shape[0]))\n",
    "    return total_acc*100./y.shape[0], adv_untargeted\n",
    "\n",
    "def l2_norms_diff(x, y):\n",
    "    x = x.reshape(x.shape[0], -1).cpu().numpy()\n",
    "    y = y.reshape(y.shape[0], -1)\n",
    "    norms = np.linalg.norm(x-y, ord=2, axis=1)\n",
    "    return norms, norms.mean(), np.median(norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Mean Norm: 0.018588968940410518\n",
      "FC Mean Norm: 0.34044947235828005\n"
     ]
    }
   ],
   "source": [
    "range_input = (data.max() - data.min())\n",
    "\n",
    "adversary = DDNL2Attack(model_cnn, loss_fn=nn.CrossEntropyLoss(reduction=\"sum\"),\n",
    "                nb_iter=1000, clip_min=data.min(), clip_max=data.max(), quantize=False,\n",
    "                targeted=False)\n",
    "robust_acc, adv_data = test_pgd_ddn(data, y, adversary, 2)\n",
    "# print(\"Epsilon : {}, Robust Accuracy: {}\".format(eps, robust_acc))\n",
    "norms, mean_norm, median_norm = l2_norms_diff(adv_data, data)\n",
    "print(\"CNN Mean Norm: {}\".format(mean_norm))\n",
    "\n",
    "adversary = DDNL2Attack(model_fc, loss_fn=nn.CrossEntropyLoss(reduction=\"sum\"),\n",
    "                nb_iter=1000, clip_min=data.min(), clip_max=data.max(), quantize=False,\n",
    "                targeted=False)\n",
    "robust_acc, adv_data = test_pgd_ddn(data, y, adversary, 2)\n",
    "# print(\"Epsilon : {}, Robust Accuracy: {}\".format(eps, robust_acc))\n",
    "norms, mean_norm, median_norm = l2_norms_diff(adv_data, data)\n",
    "print(\"FC Mean Norm: {}\".format(mean_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.norm(adv_data.reshape(data.shape[0], -1), p=2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_input = (data.max() - data.min())\n",
    "\n",
    "for eps in [0.001, 0.002, 0.005, 0.01, 0.02, 0.03, 0.04, 0.05]:\n",
    "    range_attack = range_input*eps\n",
    "    step_attack = range_attack/50\n",
    "    adversary = LinfPGDAttack(model_cnn, loss_fn=nn.CrossEntropyLoss(reduction=\"sum\"), eps=range_attack,\n",
    "                    nb_iter=100, eps_iter=step_attack, rand_init=True, clip_min=data.min(), clip_max=data.max(),\n",
    "                    targeted=False)\n",
    "    robust_acc = test_pgd(data, y, adversary, 2)\n",
    "    print(\"Epsilon : {}, Robust Accuracy: {}\".format(eps, robust_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_input = (data.max() - data.min())\n",
    "\n",
    "for eps in [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08]:\n",
    "    range_attack = range_input*eps\n",
    "    step_attack = range_attack/50\n",
    "    adversary = L2PGDAttack(model_cnn, loss_fn=nn.CrossEntropyLoss(reduction=\"sum\"), eps=range_attack,\n",
    "                    nb_iter=100, eps_iter=step_attack, rand_init=True, clip_min=data.min(), clip_max=data.max(),\n",
    "                    targeted=False)\n",
    "    robust_acc = test_pgd(data, y, adversary, 2)\n",
    "    print(\"Epsilon : {}, Robust Accuracy: {}\".format(eps, robust_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN Model <br>\n",
    "Epsilon : 0.001, Robust Accuracy: 100.0 <br>\n",
    "Epsilon : 0.002, Robust Accuracy: 100.0 <br>\n",
    "Epsilon : 0.005, Robust Accuracy: 0.0 <br>\n",
    "Epsilon : 0.01, Robust Accuracy: 0.0 <br>\n",
    "Epsilon : 0.02, Robust Accuracy: 0.0 <br>\n",
    "Epsilon : 0.03, Robust Accuracy: 0.0 <br>\n",
    "Epsilon : 0.04, Robust Accuracy: 0.0 <br>\n",
    "Epsilon : 0.05, Robust Accuracy: 0.0 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FC Model <br>\n",
    "Epsilon : 0.001, Robust Accuracy: 100.0 <br>\n",
    "Epsilon : 0.002, Robust Accuracy: 100.0 <br>\n",
    "Epsilon : 0.005, Robust Accuracy: 100.0 <br>\n",
    "Epsilon : 0.01, Robust Accuracy: 100.0 <br>\n",
    "Epsilon : 0.02, Robust Accuracy: 100.0 <br>\n",
    "Epsilon : 0.03, Robust Accuracy: 100.0 <br>\n",
    "Epsilon : 0.04, Robust Accuracy: 100.0 <br>\n",
    "Epsilon : 0.05, Robust Accuracy: 0.0 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREVIOUS EXPERIMENTS DISCARDDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FC Model <br>\n",
    "Epsilon : 0.01, Robust Accuracy: 100.0 <br>\n",
    "Epsilon : 0.02, Robust Accuracy: 100.0 <br>\n",
    "Epsilon : 0.03, Robust Accuracy: 100.0 <br>\n",
    "Epsilon : 0.04, Robust Accuracy: 100.0 <br>\n",
    "Epsilon : 0.05, Robust Accuracy: 100.0 <br>\n",
    "Epsilon : 0.06, Robust Accuracy: 50.0 <br>\n",
    "Epsilon : 0.07, Robust Accuracy: 100.0 <br>\n",
    "Epsilon : 0.08, Robust Accuracy: 100.0 <br>\n",
    "Epsilon : 0.09, Robust Accuracy: 0.0 <br>\n",
    "Epsilon : 0.1, Robust Accuracy: 100.0 <br>\n",
    "<br>\n",
    "CNN Model <br>\n",
    "Epsilon : 0.01, Robust Accuracy: 0.0 <br>\n",
    "Epsilon : 0.02, Robust Accuracy: 0.0 <br>\n",
    "Epsilon : 0.03, Robust Accuracy: 0.0 <br>\n",
    "Epsilon : 0.04, Robust Accuracy: 0.0 <br>\n",
    "Epsilon : 0.05, Robust Accuracy: 0.0 <br>\n",
    "Epsilon : 0.06, Robust Accuracy: 0.0 <br>\n",
    "Epsilon : 0.07, Robust Accuracy: 0.0 <br>\n",
    "Epsilon : 0.08, Robust Accuracy: 0.0 <br>\n",
    "Epsilon : 0.09, Robust Accuracy: 0.0 <br>\n",
    "Epsilon : 0.1, Robust Accuracy: 0.0 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_input = np.linalg.norm(data.reshape((data.shape[0], -1)), ord=2, axis=1).mean()\n",
    "\n",
    "for eps in [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]:\n",
    "    range_attack = range_input*eps\n",
    "    step_attack = range_attack/10\n",
    "    adversary = L2PGDAttack(model_cnn, loss_fn=nn.CrossEntropyLoss(reduction=\"sum\"), eps=range_attack,\n",
    "                    nb_iter=50, eps_iter=step_attack, rand_init=True, clip_min=data.min(), clip_max=data.max(),\n",
    "                    targeted=False)\n",
    "    robust_acc = test_pgd(data, y, adversary, 2)\n",
    "    print(\"Epsilon : {}, Robust Accuracy: {}\".format(eps, robust_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-fast_adversarial] *",
   "language": "python",
   "name": "conda-env-.conda-fast_adversarial-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
